\documentclass[a4paper]{article}

\def\nterm {April}
\def\nyear {2024}
\def\nlecturer {Ryan Tibshirani}
\def\ncourse {Convex Optimization: Fall 2019}

\input{header}

\begin{document}
\maketitle
{\small
\noindent\textbf{Course Information}\\ 
\indent \textbf{Instructor:} \textcolor{blue}{\href{https://www.stat.berkeley.edu/~ryantibs/index.html}{\nlecturer}} \\
\indent\textbf{Homepage:} \textcolor{blue}{\href{https://www.stat.cmu.edu/~ryantibs/convexopt/}{Machine Learning 10-725}} \\
\indent\textbf{Teaching:} Carnegie Mellon University \\

\vspace{10pt}
\noindent\textbf{Schedule}\\
\indent This course is divided into five parts: 
\begin{enumerate}
    \item \textbf{Theory I: Fundamentals}
    \begin{itemize}
        \item Introduction
        \item Convexity I: Sets and Functions
        \item Convexity II: Optimization Basics
        \item Canonical Problem Forms
    \end{itemize}
    \item \textbf{Algorithms I: First-order methods}
    \begin{itemize}
      \item Gradient Descent
    \end{itemize}
    \item \textbf{Theory II: Duality and optimality}
    \item \textbf{Algorithms II: Second-order methods}
    \item \textbf{Advanced topics}
\end{enumerate}

\tableofcontents


\section{Convexity I: Sets and Functions}
\subsection{Convex sets}
\begin{defi}[Convex set]
  $C \subseteq \R^n$ such that 
  \[
      x, y \in C \Longrightarrow  tx + (1-t)y \in C,\text{\ for\ all}\ 0 \leq t \leq 1 
  \]
\end{defi}

\begin{figure}[htbp] 
  \centering 
  \includegraphics[width=0.6\textwidth]{img/convex_set.pdf} 
\end{figure}

\begin{defi}[Convex combination] 
  For $x_1,\cdots,x_k \in \mathbb{R}^n$, any linear combination
  \[
    \theta_1 x_1 + \cdots + \theta_k x_k
    \]
  with $\theta_i \geq 0, i=1,\cdots, k$, and $\sum_{i = 1}^{k}\theta_i = 1$. 
\end{defi}

\begin{defi}[Convex hull] 
  The convex hull of $ C $, conv($C$), is all convex combinations of elements, and is always convex.
\end{defi}

\subsubsection{Example of convex sets:}
\begin{enumerate}
  \item \textbf{Trivial ones:} empty set, point, line 
  \item \textbf{Norm ball:} $\{x: \Vert x \Vert \leq r \}$, for given norm $\Vert \cdot \Vert$, radius $r$
  \item \textbf{Hyperplane:} $\{x: a^T x = b \}$, for given $a, b$
  \item \textbf{Halfspace:} $\{x: a^T x \leq b \}$, for given $a, b$
  \item \textbf{Affine space:} $\{x: A x = b \}$, for given $A, b$
  \item \textbf{Polyhedron:} $\{x: A x \leq b \}$, while inequality $\leq$ is interpreted componentwise. Note: the set $\{x: A x \leq b, Cx = d \}$ is also a Polyhedron
  \item \textbf{Simplex:} special case of polyhedra, given by conc$\{x_0,\cdots,x_k \}$, where these points are affinely independent. The canonical example is the \textbf{probability simplex},
  \[
    \text{conv}\{ e_1,\cdots,e_n\} = \{w: w \geq 0, 1^T w = 1\}
    \]
\end{enumerate}

\subsubsection{Key properties of convex sets:}
\begin{enumerate}
  \item \textbf{Separating hyperplane theorem:} two disjoint convex sets have a separating between hyperplane them
  Formally, if $C, D$ are nonempty convex sets with $C\cap  D = \emptyset $, then there exists $a, b$ such that
  \begin{align*}
    C & \subseteq \{x : a^T x \leq b\} \\
    D & \subseteq \{x : a^T x \geq b\} 
  \end{align*}
  \item \textbf{Supporting hyperplane theorem:} a boundary point of a convex set has a supporting hyperplane passing through it. Formally, if $C$ is a nonempty convex set, and $x_0 \in \text{bd}(C)$, then there exists $a$ such that 
  \begin{equation}
    C \subseteq \{x : a^T x \leq a^T x_0\}  \nonumber 
  \end{equation}
\end{enumerate}

\begin{figure}[htbp] 
  \centering 
  \includegraphics[width=0.4\textwidth]{img/hyperplane_separating.pdf} 
\end{figure}

\subsubsection{Operations preserving convexity}
\begin{enumerate}
  \item \textbf{Intersection:} the intersection of convex sets is convex
  \item \textbf{Scaling and translation:} if $C$ is convex, then
  \[ 
    aC + b = \{ax+ b : x \in C\}
    \]
  is convex for any $a, b$
  \item \textbf{Affine images and preimages:} if $f(x) = Ax +b$ and $C$ is convex then 
  \begin{equation}
    f(C) = \{f(x) : x \in C\} \nonumber
  \end{equation}
  is convex, and if $D$ is convex then 
  \begin{equation}
    f^{-1}(D) = \{x: f(x) \in D\} \nonumber
  \end{equation}
  is convex
  \item \textbf{Perspective images and preimages:} the perspective function is $P : \mathbb{R}^n \times \mathbb{R}_{++} \rightarrow \mathbb{R}^n$ where $ \mathbb{R}_{++}$ denotes positive reals,
  \[ 
    P(x, z) = x / z
    \]
  for $ z > 0$. If $C \subseteq \text{dom}(P)$ is convex then so is $P(C)$, and if $D $ is convex then so is $P^{-1}(D)$
  \item  \textbf{Linear-fractional images and preimages:} the perspective map composed with an affine function,
  \[ 
    f(x) = \frac{Ax + b}{c^T x + d}
    \]
  is called a linear-fractional function, defined on $c^T x + d > 0$. If $C \subseteq \text{dom}(f)$ is convex then so if $f(C)$, and if $D$ is convex then so is $f^{-1}(D)$ 
\end{enumerate}

\subsection{Cones}
\begin{defi}[Cone]
  $C \subseteq \mathbb{R}^n$ such that  
  \[
    x \in C \Longrightarrow  tx \in C,\text{\ for\ all}\ t \geq 0 
  \]
\end{defi}

\begin{defi}[Convex cone]
  Cone that is also convex, i.e.,
  \[
    x_1, x_2 \in C \Longrightarrow  t_1 x_1 + t_2 x_2 \in C,\text{\ for\ all}\ t_1, t_2 \geq 0 
  \]
\end{defi}

\begin{defi}[Conic combination]
  For $x_1,\cdots,x_k \in \mathbb{R}^n$, any linear combination
  \[
    \theta_1 x_1 + \cdots + \theta_k x_k
    \]
  with $\theta_i \geq 0, i=1,\cdots, k$.
\end{defi}

\begin{defi}[Conic hull]
  The conic hull of $ C $, cone($C$), is all conic combinations of elements.
\end{defi}

\subsubsection{Example of convex cones:}
\begin{enumerate}
  \item \textbf{Norm cone:} $\{(x,t) : \Vert x \Vert \leq r \}$, for a norm $\Vert \cdot \Vert$. Under the $\ell_2$ norm $\Vert \cdot \Vert_2$, called second-order cone
  \item \textbf{Normal cone:} given any set $C$ and point $x \in C$, we can define
  \[
    \mathcal{N}_C (x) = \{g : g^T x \geq g^T y, \text{\ for \ all} \ y \in C\} 
    \]
  This is always a convex cone, regardless of $C$
  \item \textbf{Positive semidefinite cone:} $\mathbb{S}^n_+ = \{X \in \mathbb{S}^n : X \succeq 0 \}$, where $X \succeq 0 $ means that $X$ is positive semidefinite (and $\mathbb{S}^n$ is the set of $n \times n$ symmetric matrices)
\end{enumerate}

\subsection{Convex function}
\begin{defi}[Convex function]
  $f: \R^n \rightarrow \R$ such that $\dom (f) \subseteq \R^n$ convex, and 
  \[
      f(tx + (1-t)y) \leq tf(x) + (1-t)f(y),\text{\ for\ all}\ 0 \leq t \leq 1 
  \]
  and all $x, y \in \dom (f)$
\end{defi}

\begin{center}
\begin{tikzpicture}
  \draw (0.27, 0.5) -- (3, 1);
  \draw (0, 1) .. controls (1,-1) .. (3.5, 1.5);
  \node  at (0.27, 0.5) [circ] {}; 
  \node at (0.27, 0.5) [left] {$(x, f(x))$};
  \node at (3, 1) [circ] {};
  \node at (3, 1) [right] {$(y, f(y))$};
\end{tikzpicture}
\end{center}

\begin{defi}[Concave function] 
$f: \R^n \rightarrow \R$ such that $\dom (f) \subseteq \R^n$ convex, and 
\[
    f(tx + (1-t)y) \geq tf(x) + (1-t)f(y),\text{\ for\ all}\ 0 \leq t \leq 1 
\]
and all $x, y \in \dom (f)$, so that 
\[
  f \ \text{concave} \Leftrightarrow -f \ \text{convex}
\]
\end{defi}

\noindent \textbf{Important modifiers:}
\begin{enumerate}
\item \textbf{Strictly convex:} $f(tx + (1-t)y) < tf(x) + (1-t)f(y),\text{\ for} \ x \neq y $ and $ 0 < t < 1$. In words, $f$ is convex and has greater curvature than a linear function
\item \textbf{Strongly convex:} with parameter  $m > 0: f - \frac{m}{2} \Vert x \Vert_2^2$ is convex. In words, $f$ is at least as convex as a quadratic function
\item \textbf{Note:} strongly convex $\Rightarrow $ strictly convex $\Rightarrow$ convex (Analogously for concave functions)
\end{enumerate}

\subsubsection{Example of convex functions}
\begin{enumerate}
  \item \textbf{Univariate functions:}
  \begin{enumerate}
    \item Exponential function: $e^{ax}$ is convex for any $a$ over $\mathbb{R}$
    \item Power function: $x^a$ is convex for $a \geq 1$ or $a \leq 0$ over $\mathbb{R}_+$ (nonnegative reals)
    \item Power function: $x^a$ is concave for $0 \leq a \leq 1$ over $\mathbb{R}_+$
    \item Logarithmic function: $\log (x)$ is concave over $\mathbb{R}_{++}$  
  \end{enumerate}
  \item \textbf{Affine function:} $a^T x + b$ is both convex and concave
  \item \textbf{Quadratic function:} $\frac{1}{2} x^T Q x + b^T x + c$ is convex provided that $Q \succeq 0$ (positive semidefinite)
  \item \textbf{Least squares loss:} $\Vert y - Ax \Vert^2_2$ is always convex (since $A^T A$ is always positive semidefinite) 
  \item \textbf{Norm:} $\Vert x \Vert$ is convex for any norm; e.g. $\ell_p$ norms,
  \[
    \Vert x \Vert_p = (\sum_{i=1}^n |x_i|^p)^{1/p} \ \text{for} \ p \geq 1, \quad \Vert x \Vert_\infty = \max\limits_{i=1,\cdots.n}|x_i|
  \]
  and also operator (spectral) and trace (nuclear) norms,
  \[
    \Vert X \Vert_{op} = \sigma_1(X), \Vert X \Vert_{tr} = \sum_{i=1}^r \sigma_r(X)
  \]
  where $\sigma_1(X) \geq \cdots \geq \sigma_r(X) \geq 0$ are the singular values of the matrix $X$
  \item \textbf{Indicator function:} if $C$ is convex, then its indicator function
  \[
    I_C(x) = 
    \begin{cases}
      0 &\ x \in C \\
      \infty &\ x \notin C
    \end{cases}
  \]
  \item \textbf{Support function:} for any set $C$(convex or not), its indicator function 
  \[
    I_C^*(x) = \max\limits_{y \in C} x^T y  
  \]
  is convex 
  \item \textbf{Max function:} $f(x) = \max \{ x_1, \cdots, x_n \}$ is convex 

\end{enumerate}

\subsubsection{Key properties of convex functions}
\begin{enumerate}
  \item A function is convex if and only if its restriction to any line is convex
  \item \textbf{Epigraph characterization:} a function $f$ is convex if and only if its epigraph
  \[
    \text{epi}(f) = \{(x,t) \in \text{dom}(f) \times \mathbb{R} : f(x) \leq t \}
  \]
  is a convex set
  \item \textbf{Convex sublevel sets:} if $f$ is convex, then its sublevel sets 
  \[
    \{x \in \text{dom}(f) : f(x) \leq t \}
  \]
  are convex, for all $t \in \mathbb{R}$. The converse is not true
  \item \textbf{First order characterization:} if $f$ is differentiable, then $f$ is convex if and only if dom($f$) is convex, and
  \[
    f(y) \geq f(x) + \nabla f(x)^T(y-x)
  \]
  for all $x, y \in $ dom($f$). Therefore for a differentiable convex function $ \nabla f(x) = 0 \Leftrightarrow x$ minimizes $f$
  \item \textbf{Second order characterization:} if $f$ is twice differentiable, then $f$ is convex if and only if dom($f$) is convex, and $\nabla^2 f(x) \succeq 0$ for all $x \in $ dom($f$)
  \item \textbf{Jensen's inequality:} if $f$ is convex, and $X$ is a random variable supported on dom($f$), then $f(\mathbb{E}[X]) \leq \mathbb{E}[f(x)]$
\end{enumerate}

\subsubsection{Operations preserving convexity}
\begin{enumerate}
  \item \textbf{Nonnegative linear combination:} $f_1,\cdots,f_m$ convex implies $a_1f_1 + \cdots + a_mf_m$ convex for any $a_1,\cdots,a_m \geq 0$
  \item \textbf{Pointwise maximization:} if $f_s$ is convex for any $s \in S$,then $f(x) = \max_{s \in S} f_s(x)$ is convex. Note that the set $S$ here (number of functions $f_s$) can be infinite
  \item \textbf{Partial minimization:} if $g(x,y)$ is convex in $x, y$, and $C$ is convex, then $f(x) = \min_{y \in C}g(x,y)$ is convex
  \item \textbf{Affine composition:} if $f$ is convex, then $g(x)=f(Ax+b)$ is convex
  \item \textbf{General composition:} suppose $f = h\circ g$, where $g : \mathbb{R}^n \rightarrow \mathbb{R}$, $h : \mathbb{R} \rightarrow \mathbb{R}$, $f : \mathbb{R}^n \rightarrow \mathbb{R}$. Then:
  \subitem $f$ is convex if $h$ is convex and nondecreasing, $g$ is convex
  \subitem $f$ is convex if $h$ is convex and nonincreasing, $g$ is concave
  \subitem $f$ is concave if $h$ is concave and nondecreasing, $g$ is concave
  \subitem $f$ is concave if $h$ is concave and nonincreasing, $g$ is convex
  \item \textbf{Vector composition:} suppose that
  \[
    f(x) = h(g(x)) = h(g_1(x), \cdots, g_k(x))
  \]
  where $g : \mathbb{R}^n \rightarrow \mathbb{R}^k$, $h : \mathbb{R}^k \rightarrow \mathbb{R}$, $f : \mathbb{R}^n \rightarrow \mathbb{R}$. Then:
  \subitem $f$ is convex if $h$ is convex and nondecreasing in each argument, $g$ is convex
  \subitem $f$ is convex if $h$ is convex and nonincreasing in each argument, $g$ is concave
  \subitem $f$ is concave if $h$ is concave and nondecreasing in each argument, $g$ is concave
  \subitem $f$ is concave if $h$ is concave and nonincreasing in each argument, $g$ is convex
\end{enumerate}

\section{Convexity II: Optimization Basics}
\subsection{Optimization terminology}
\begin{figure}[htbp] 
  \centering 
  \includegraphics[width=0.6\textwidth]{img/convex_vs_nonconvex.pdf} 
\end{figure}

\begin{defi}[Optimization problem] 
    \begin{align*}
        \min _{x \in D} & \quad f(x) \\
        \text { subject to } &\quad g_{i}(x) \leq 0, \ i=1, \ldots, m \\
        &\quad h_{j}(x)=0, \ j=1, \ldots, r
    \end{align*}
  here $D = \dom (f) \cap \bigcap_{i=1}^{m} \dom (g_{i}) \cap \bigcap_{j=1}^{r} \dom(h_j)$, common domain of all functions.
\end{defi}

\begin{defi}[Convex optimization problem] 
  \begin{align*}
      \min _{x \in D} & \quad f(x)  \\
      \text { subject to } &\quad g_{i}(x)\leq 0, \ i=1, \ldots, m \\
      & \quad  Ax = b 
  \end{align*}
  where $f$ and $g_i, i= 1,\cdots,m $ are all convex, and the optimization domain is $D = \dom (f) \cap \bigcap_{i=1}^{m} \dom (g_{i})$.
\end{defi}

% This is a \textbf{convex optimization problem} provided the functions $f$ and $g_i, i=1,\ldots,m$ are convex, and  $h_j, j=1,\ldots,r$ are affine:
% \[
%     h_j (x) = a_{j}^{T}x + b_j, \quad j =1,\ldots,r
% \]

\noindent For convex optimization problem, \textbf{local minima are global minima}.
Formally, if $x$ is feasible---$x \in D$, and satisfies all constraints and minimizes $f$ in a local neighborhood,
\[
    f(x) \leq f(y) \text{\ for \ all \ feasible} \ y, \ \left\lVert x -y \right\rVert_2 \leq \rho
\]
then
\[
    f(x) \leq f(y) \text{\ for \ all \ feasible} \ y
\]


\noindent \textbf{Terminologies:}
\begin{enumerate}
  \item $f$ is called criterion or objective function
  \item $g_i$ is called inequality constraint function
  \item If $x \in D, g_i(x) \leq 0, i= 1,\cdots, m$, and $Ax = b$ then $x$ is called a feasible point
  \item The minimum of $f(x)$ over all feasible points $x$ is called the optimal value, written $f^*$
  \item If $x$ is feasible and $f(x) = f^*$, then $x$ is called optimal; also called a solution, or a minimizer
  \item If $x$ is feasible and $f(x) \leq f^* + \epsilon$, then $x$ is called $\epsilon$-suboptimal
  \item If $x$ is feasible and $g_i(x) = 0$, then we say $g_i$ is active at $x$
  \item Convex minimization can be reposed as concave maximization, i.e.  min $f(x) \Leftrightarrow$ max $-f(x)$, both are called convex optimization problems
  \item the optimization problem can be rewritten as 
  \[
    \min_x f(x) \quad \text{subject to} \ x \in C
  \]
  where $C$ is the feasible set. Hence the formulation is complete general. With $I_C$ the indicator of $C$, it can also be written as the unconstrained form
  \[
    \min_x \ f(x) + I_C(x)
  \]
\end{enumerate}

\begin{defi}[Solution set] 
  Let $X_{opt}$ be the set of all solutions of convex problem, written 
  \begin{align*}
    X_{opt} = & \arg \min  \quad  \quad \ f(x) \\
      & \text { subject to }  \quad g_{i}(x) \leq 0, \ i=1, \ldots, m \\
      & \quad  \quad \quad \quad  \quad \quad Ax = b 
  \end{align*}
\end{defi}
\noindent \textbf{Two key properties:}
\begin{enumerate}
  \item $X_{opt}$ is a convex set, and it can be proofed using definitions. If $x, y$ are solutions, then for $0 \leq t \leq 1$, $t(x + (1-t)y)$ is also a solution
  \item If $f$ is strictly convex, then solution is unique, i.e., $X_{opt}$ contains one element 
\end{enumerate}

\subsection{First-order optimality condition}
\begin{defi}[First-order optimality condition]
  For a convex problem
  \[
    \min_x f(x) \quad \text{subject to} \ x \in C
  \]
  and differentiable $f$, a feasible point $x$ is optimal if and only if 
  \[
    \nabla f(x)^T (y-x)\geq 0  \quad \text{for all} \ y \in C
  \]
this means all feasible directions from $x$ are aligned with gradient $\nabla f(x)$
\end{defi}

\noindent \textbf{Important case:} if $C \in \mathbb{R}^n$ (unconstrained optimization), then optimality condition reduces to familiar $\nabla f(x) = 0$.

\subsubsection{Example: quadratic minimization}
Consider minimizing the quadratic function
\begin{equation}
  f(x) = \frac{1}{2}x^T Q x + b^T x + c \nonumber
\end{equation}
where $Q \succeq 0$. The first-order condition says that solution satisfies
\begin{equation}
  \nabla f(x) = Q x + b = 0 \nonumber
\end{equation}
\begin{enumerate}
  \item if $Q \succeq 0$, then there is a unique solution $x = -Q ^{-1}b$
  \item if $Q $ is singular and $b \notin \text{col}(Q)$, then there is no solution (i.e., $\text{min}_x \ f(x) = - \infty$)
  \item if $Q $ is singular and $b \in \text{col}(Q)$, then there are infinitely many solutions
  \begin{equation}
    x = -Q^+ b + z, \quad z \in \text{null}(Q) \nonumber
  \end{equation}
  where $Q^+$ is the pseudoinverse of $Q$. 
  
  \textbf{Note:} $\text{null}(Q) = \{z \in C^n : Qz = 0 \} $
\end{enumerate}

\subsubsection{Example: equality-constrained minimization}
Consider the equality-constrained convex problem
\begin{equation}
  \min_x f(x) \quad \text{subject to} \ Ax = b \nonumber
\end{equation}
with $f$ differentiable. Let's prove Lagrange multiplier optimality condition
\begin{equation}
  \nabla f(x) + A^T u = 0 \quad \text{for some} \ u \nonumber
\end{equation}
According to first-order optimality, solution $x$ satisfies $Ax = b$ and 
\begin{equation}
  \nabla f(x)^T (y-x) \geq 0 \quad \text{for all} \ y \ \text{such that} \ Ay = b \nonumber
\end{equation}
This is equivalent to 
\begin{equation}
  \nabla f(x)^T v = 0  \ \text{for all} \ v \in \text{null}(A) \nonumber
\end{equation}
Result follows because $\text{null}(A)^{\perp} = \text{row}(A)$.

\subsubsection{Example: projection onto a convex set}
Consider projection onto convex set $C$
\begin{equation}
  \min_x \Vert a- x \Vert_2^2 \quad \text{subject to} \ x \in C \nonumber
\end{equation}
First-order optimality condition says that the solution $x$ satisfies 
\begin{equation}
  \nabla f(x)^T (y-x) = (x - a)^T(y-x) \geq 0 \quad \text{for all} \ y \in C \nonumber
\end{equation}
Equivalent, this says that
\begin{equation}
  a - x \in \mathcal{N}_C (x) \nonumber
\end{equation}
where recall $\mathcal{N}_C (x) $ is the normal cone to $C$ at $x$. 

\subsection{Partial optimization}
We can always partial optimize a convex problem and retain convexity, e.g.
\begin{equation}
  \begin{array}{ll}
    \ \min\limits_{x_{1}, x_{2}} & f\left(x_{1}, x_{2}\right) \\
    \text{ subject to } & g_{1}\left(x_{1}\right) \leq 0 \\
    & g_{2}\left(x_{2}\right) \leq 0
  \end{array}
  \Longleftrightarrow 
  \begin{array}{ll}
    \ \min\limits_{x_{1}} & \tilde{f}\left(x_{1}\right) \\
    \text { subject to } & g_{1}\left(x_{1}\right) \leq 0 \nonumber \\
    {} & {}
  \end{array}
\end{equation}
where $\tilde{f}\left(x_{1}\right) = \min\{ f(x_1, x_2) : g_2(x_2) \leq 0\}$. The right problem is convex if the left problem is.

\subsubsection{Excample: hinge form of SVMs}
Recall the SVM problem
\begin{equation}
  \begin{array}{ll}
    \min\limits_{\beta, \beta_{0}, \xi} & \dfrac{1}{2}\Vert\beta\Vert_{2}^{2}+C \sum\limits_{i=1}^{n} \xi_{i} \\\text { subject to } & \xi_{i} \geq 0, y_{i}\left(x_{i}^{T} \beta+\beta_{0}\right) \geq 1-\xi_{i}, i=1, \ldots, n \nonumber
  \end{array}
\end{equation}
Rewrite the constraints as $\xi_i \geq \max\{0, 1 - y_{i}\left(x_{i}^{T} \beta+\beta_{0}\right)\}$. Indeed we can argue that we have $=$ at solution. Therefor plugging in for optimal $\xi$ gives the hinge form of SVMs:
\begin{equation}
  \min\limits_{\beta, \beta_{0}}\dfrac{1}{2}\Vert\beta\Vert_{2}^{2}+C \sum\limits_{i=1}^{n} [ 1 - y_{i}\left(x_{i}^{T} \beta+\beta_{0}\right) ]_+ \nonumber
\end{equation}
where $a_+ = \max \{0, a\}$ is called the hinge function

\subsection{Transformations and change of variables}
If $h : \mathbb{R} \rightarrow \mathbb{R}$ is a monotone increasing translation, then
\begin{equation}
  \min_x f(x) \quad \text{subject to} \ x \in C \ \Longleftrightarrow \ \min_x h(f(x)) \quad \text{subject to} \ x \in C \nonumber
\end{equation}
Similarly, inequality or equality constraints can be transformed and yield equivalent optimization problem. This means we can use this to reveal the `hidden convexity' of a problem.

\noindent If $\phi : \mathbb{R}^n \rightarrow \mathbb{R}^m$ is one-to-one, and its image covers feasible set $C$, then we can change variables in an optimization problem:
\begin{equation}
  \min_x f(x) \quad \text{subject to} \ x \in C \ \Longleftrightarrow \ \min_y f(\phi(y)) \quad \text{subject to} \ \phi(y) \in C \nonumber
\end{equation}

\subsubsection{Example: geometric programming}
A monomial is a function $f : \mathbb{R}^n_{++} \rightarrow \mathbb{R}$ of the form
\begin{equation}
  f(x) = \gamma x_1^{a_1}x_2^{a_2}\cdots x_n^{a_n} \nonumber
\end{equation}
for $\gamma > 0, a_1, \cdots, a_n \in \mathbb{R}$. A posynomial is a sum of monomials,
\begin{equation}
  f(x) = \sum_{k=1}^{p}\gamma_k x_1^{a_k1}x_2^{a_k2}\cdots x_n^{a_kn} \nonumber
\end{equation}

\noindent A geometric program is of the form
\begin{equation}
  \begin{array}{ll}
    \min\limits_{x} & \quad f(x)  \\
    \text { subject to } &\quad g_{i}(x)\leq 0, \ i=1, \ldots, m \\ \nonumber
    & \quad h_j(x) = 1, \ j = 1, \cdots, r
\end{array}
\end{equation}
where $f, g_i, i = 1, \cdots, m$ are posynomials and $h_j, j = 1,\cdots, r$ are monomials. This is nonconvex.

\noindent Given $f(x) = \gamma x_1^{a_1}x_2^{a_2}\cdots x_n^{a_n}$, let $y_i = \log x_i$ and rewrite this as 
\begin{equation}
  \gamma (e^{y_1})^{a_1} (e^{y_2})^{a_2}  \cdots (e^{y_n})^{a_n} = e^{a^T y + b} \nonumber 
\end{equation}
for $ b = \log \gamma$. Also, a posynomial can be written as $\sum_{k=1}^p e^{a_k^T y + b_k}$. With this variable substitution, and after taking logs, a geometric program is equivalent to
\begin{equation}
  \begin{array}{ll}
  \min\limits_x  & \log(\sum\limits_{k=1}^{p_0} e^{a_{0k}^T y + b_{0k}}) \\
  \text{subject to}  &  \log(\sum\limits_{k=1}^{p_i} e^{a_{ik}^T y + b_{ik}}) \leq 0, \ i = 1, \cdots, m \\ \nonumber
  & c_j^T y + d_j = 0, \ j = 1, \cdots, r
\end{array}
\end{equation} 
This is convex, recalling the convexity of soft max functions.

\subsection{Eliminating equality constraints}
Important special case of change of variables: eliminating equality constraints. Given the problem 
\begin{equation}
  \begin{array}{ll}
    \min\limits_{x} & \quad f(x)  \\
    \text { subject to } &\quad g_{i}(x)\leq 0, \ i=1, \ldots, m \\ \nonumber
    & \quad Ax = b
\end{array}
\end{equation}
we can always express any feasible point as $x = My + x_0$, where $Ax_0 = b$ and $\text{col}(M) = \text{null}(A)$. Hence the above is equivalent to 
\begin{equation}
  \begin{array}{ll}
    \min\limits_{y} & \quad f(My + x_0)  \\
    \text { subject to } &\quad g_{i}(My + x_0)\leq 0, \ i=1, \ldots, m \\ \nonumber
\end{array}
\end{equation}
\textbf{Note:} this is fully general but not always a good idea (practically).

\subsection{Introducing slack variables}
Essentially opposite to eliminating equality constraints: introducing slack variables. Given the problem
\begin{equation}
  \begin{array}{ll}
    \min\limits_{x} & \quad f(x)  \\
    \text { subject to } &\quad g_{i}(x)\leq 0, \ i=1, \ldots, m \\ \nonumber
    & \quad Ax = b
\end{array}
\end{equation}
we can transform the inequality constraints via
\begin{equation}
  \begin{array}{ll}
    \min\limits_{x, s} & \quad f(x)  \\
    \text { subject to } & \quad s_i \geq 0, \ i=1, \ldots, m \\ \nonumber
    & \quad g_{i}(x) + s_i = 0, \ i=1, \ldots, m \\
    & \quad Ax = b
\end{array}
\end{equation}
\textbf{Note:} this is no longer convex unless $g_i, i= 1, \cdots, n$ are affine.

\subsection{Relaxing nonaffine equalities}
Given an optimization problem
\begin{equation}
  \min_x f(x) \quad \text{subject to} \ x \in C \nonumber
\end{equation}
we can always take an enlarged constraint set $\tilde{C} \supseteq C$ and consider 
\begin{equation}
  \min_x f(x) \quad \text{subject to} \ x \in \tilde{C} \nonumber
\end{equation}
This is called a relaxation and its optimal value is always smaller or equal to that of the original problem. An important special case: relaxing nonaffine equality constraints, i.e., 
\begin{equation}
  h_j(x) = 0, \ j=1,\cdots, r \nonumber
\end{equation}
where $h_j, j = 1,\cdots, r$ are convex but nonaffine, are replaced with
\begin{equation}
  h_j(x) \leq 0, \ j=1,\cdots, r \nonumber
\end{equation}

\subsubsection{Example: maximum utility problem}
The maximum utility problem models investment/consumption:
\begin{equation}
  \begin{array}{ll}
    \min\limits_{x, b} & \quad \sum_{t=1}^T \alpha_t u(x_t)  \\
    \text { subject to } &\quad b_{t+1} = b_t + f(b_t) - x_t, \ t=1, \ldots, T \\ \nonumber
    & 0 \leq x_t \leq b_t, t = 1, \cdots, T
\end{array}
\end{equation}
Here $b_t$ is the budget and $x_t$ is the amount consumed at time $t$; $f$ is an investment return function, $u$ utility function, both concave and increasing (Here, $f(0) =0$, and $b_0 > 0$ is given).

\subsubsection{Example: principal components analysis}
Given $X \in \mathbb{R}^{n \times p}$, consider the low rank approximation problem:
\begin{equation}
  \min_R \Vert X - R \Vert^2_F \quad \text{subject to} \ \rank(R) = k \nonumber
\end{equation} 
Here $\Vert A\Vert^2_F = \sum_{i=1}^n \sum_{j=1}^p A_{ij}^2$, the entrywise squared $\ell_2$ norm, and $\rank(A)$ denotes the rank of $A$. The problem is also called principal components analysis or PCA problem. Given $X = UDV^T$, singular value decomposition or SVD, the solution is
\begin{equation}
  R = U_k D_k V_k^T \nonumber
\end{equation}
where $U_k, V_k$ are the first $k$ columns of $U, V$ and $D_k$ is the first $k$ diagonal elements of $D$. That is, $R$ is reconstruction of $X$ from its first $k$ principal components.

The PCA problem is not convex, but we can recast it. First rewrite as
\begin{equation}
  \begin{aligned}
  \min_{Z \in \mathbb{S}^p} \Vert X - XZ \Vert^2_F \quad \text{subject to} \ \rank(Z) = k, Z \ \text{is a projection} \\
  \Longleftrightarrow \max_{Z \in \mathbb{S}^p} tr(SZ) \quad \text{subject to} \ \rank(Z) = k, Z \ \text{is a projection} \nonumber 
  \end{aligned}
\end{equation} 
where $S = X^T X$. Hence constraint set is the nonconvex set
\begin{equation}
  C = \{ Z \in   \mathbb{S}^p : \lambda_i(Z) \in \{0, 1\}, i = 1,\cdots, p, \tr(Z) = k \} \nonumber
\end{equation}
where $\lambda_i(Z), i = 1,\cdots, n$ are the eigenvalues of $Z$. Solution in this formulation is 
\begin{equation}
  Z = V_k V_k^T \nonumber
\end{equation}
where $V_k$ gives first $k$ columns of $V$.

\noindent Now consider relaxing constraint set to $\mathcal{F}_k = \text{conv}(V)$, its convex hull. 

\noindent \textbf{Note:} 
\begin{equation}
  \begin{aligned}
  \mathcal{F}_k & = \{ Z \in   \mathbb{S}^p : \lambda_i(Z) \in [0, 1], i = 1,\cdots, p, \tr(Z) = k \} \nonumber \\
   & = \{ Z \in   \mathbb{S}^p : 0 \preceq Z \preceq I, \tr(Z) = k \} 
  \end{aligned}
\end{equation}
This set is called the Fantope of order $k$ and it is convex. Hence, the linear maximization over the Fantope, namely
\begin{equation}
  \max_{Z \in  \mathcal{F}_k} \quad \tr(SZ) \nonumber
\end{equation}
is a convex problem. Remarkably, this is equivalent to the original nonconvex PCA problem (admit the same solution).

\section{Canonical Problem Forms}
The relationship among linear programs (LPs), quadratic programs (QPs), semidefinite programs (SDPs), second-order cone program (SOCPs) and cone programs is shown in the following figure.
\begin{figure}[htbp] 
  \centering 
  \includegraphics[width=0.6\textwidth]{img/LPs_relationship.pdf} 
\end{figure}

\subsection{Linear program}
\begin{defi}[Linear program]
  A linear program is an optimization problem of the form
  \[
  \begin{array}{ll}
      \min\limits_{x} & c^T x  \\
      \text { subject to } & Dx \leq d \\ \nonumber
      & Ax = b
  \end{array} 
  \]
  and this is always a convex optimization problem.
\end{defi}

\noindent The standard form of LP is 
\begin{equation}
  \begin{array}{ll}
    \min\limits_{x} & c^T x  \\
    \text { subject to } & x \geq 0 \\ \nonumber
    & Ax = b
\end{array} 
\end{equation}
any LP can be rewritten in standard form.

\subsubsection{Excample: basis pursuit}
Given $y \in \mathbb{R}^n$ and $X \in \mathbb{R}^{n \times p}$, where $p > n$. Suppose that we seek the sparest solution to underdetermined linear system $X \beta = y$. If using $\ell_0$ norm, it will be a nonconvex formulation. The $\ell_1$ approximation, often called basis pursuit
\begin{equation}
  \begin{array}{ll}
    \min\limits_{\beta} & \Vert \beta \Vert_1  \\ \nonumber
    \text { subject to } & X \beta = y \\ 
\end{array} 
\end{equation}
This problem can be reformulated as the LP form 
\begin{equation}
  \begin{array}{ll}
    \min\limits_{\beta, z} & 1^T z  \\ \nonumber
    \text { subject to } & z \geq \beta \\
    & z \geq -\beta \\
    & X \beta = y
\end{array} 
\end{equation}

\subsubsection{Excample: Dantzig selector}
Modification of previous problem, where we allow for $X \beta \approx y$, the Dantzig selector
\begin{equation}
  \begin{array}{ll}
    \min\limits_{\beta} & \Vert \beta \Vert_1  \\ \nonumber
    \text { subject to } & \vert X^T (y-X \beta)\Vert_\infty \leq \lambda \\ \nonumber
\end{array} 
\end{equation}
where $\lambda \geq 0$ is a tuning parameter. It can also be reformulated as a 
LP.

\subsection{Convex quadratic program}
\begin{defi}[Convex quadratic program]
  A convex quadratic program is an optimization problem of the form
  \[
  \begin{array}{ll}
      \min\limits_{x} & c^T x + \dfrac{1}{2}x^T Qx \\
      \text { subject to } & Dx \leq d \\ \nonumber
      & Ax = b
  \end{array} 
  \]
  where $Q \succeq  0$. Note that this problem is not convex when $Q \nsucceq 0$. From now on, when we say QP, we implicitly assume that $Q \succeq 0$.
\end{defi}

\noindent The standard form of QP is 
\begin{equation}
  \begin{array}{ll}
    \min\limits_{x} & c^T x + \dfrac{1}{2}x^T Qx \\
    \text { subject to } & x \geq 0 \\ \nonumber
    & Ax = b
\end{array} 
\end{equation}
any QP can be rewritten in standard form.

\subsubsection{Example: support vector machines}
Given $y \in \{-1, 1\}^n$, $X \in \mathbb{R}^{n \times p}$ having rows $x_1, \cdots, x_n$, recall the SVM problem
\begin{equation}
  \begin{array}{ll}
    \min\limits_{\beta, \beta_{0}, \xi} & \dfrac{1}{2}\Vert\beta\Vert_{2}^{2}+C \sum\limits_{i=1}^{n} \xi_{i} \\\text { subject to } & \xi_{i} \geq 0, y_{i}\left(x_{i}^{T} \beta+\beta_{0}\right) \geq 1-\xi_{i}, i=1, \ldots, n \nonumber
  \end{array}
\end{equation}

\subsection{Semidefinite program}
\begin{defi}[Semidefinite program]
  A semidefinite program is an optimization problem of the form
  \[
  \begin{array}{ll}
      \min\limits_{x} & c^T x \\
      \text { subject to } & x_1 F_1 + \cdots x_n F_n \succeq F_0 \\ \nonumber
      & Ax = b
  \end{array} 
  \]
  where $F_j \in \mathbb{S}^d$, for $j = 0,1,\cdots, n$, and $A \in \mathbb{R}^{m \times n}, c \in \mathbb{R}^{n}, b \in \mathbb{R}^{m}$.
\end{defi}

\noindent The standard form of SDP is 
\begin{equation}
  \begin{array}{ll}
    \min\limits_{X} & C \bullet X \\
    \text { subject to } & A_i \bullet X = b_i, i= 1,\cdots, m \\ \nonumber
    &X \succeq 0
\end{array} 
\end{equation}
where $X \bullet Y  = \tr (XY)$ and any SDP can be rewritten in standard form.

\noindent \textbf{Note:} $\mathbb{S}^n$ is space of $n \times n$ symmetric matrices, $\mathbb{S}^n_+$ is the space of positive semidefinite matrices, i.e.,
\begin{equation}
  \mathbb{S}^n_+ = \{ X \in \mathbb{S}^n : u^TXu \geq 0 \ \text{for all} \ u \in \mathbb{R}^n \} \nonumber
\end{equation}
$\mathbb{S}^n_{++}$ is the space of positive define matrices, i.e.,
\begin{equation}
  \mathbb{S}^n_{++} = \{ X \in \mathbb{S}^n : u^TXu > 0 \ \text{for all} \ u \in \mathbb{R}^n \setminus \{ 0\} \} \nonumber
\end{equation}

\subsubsection{Example: theta function}
Let $G = (N, E)$ be an undirected graph, $N = \{1,\cdots, n\}$, $\omega(G)$ and $\chi (G)$ is the clique number and chromatic number of $G$, respectively. The Lovasz theta function is 
\begin{equation}
  \begin{array}{ll}
    \vartheta (G) = \max\limits_{X} & 11^T \bullet X \\
    \text { subject to } & I \bullet X = 1\\ \nonumber
    & X_{ij} = 0, (i,j) \notin E \\
    & X \succeq 0
\end{array} 
\end{equation}
The Lovasz sandwich theorem: $\omega(G) \leq \vartheta(\overline{G}) \leq \chi (G)$, where $\overline{G} $ is the complement graph of $G$.

\subsubsection{Example: trace norm minimization}
Let $A: \mathbb{R}^{m \times n} \rightarrow \mathbb{R}^p$ be a linear map
\begin{equation}
  A(X)=\left(\begin{array}{c}
    A_{1} \bullet X \\
    \cdots \\ \nonumber
    A_{p} \bullet X
    \end{array}\right)
\end{equation}
for $A_1, \cdots, A_p \in \mathbb{R}^{m \times n}$ and $A_i \bullet X = \tr(A^T_i X)$. Finding lowest-rank solution to an underdetermined system, nonconvex
\begin{equation}
  \begin{array}{ll}
     \min\limits_{X} & \rank(X) \\
    \text { subject to } & A(X) = b\\ \nonumber
\end{array} 
\end{equation}
Trace norm approximation:
\begin{equation}
  \begin{array}{ll}
     \min\limits_{X} & \Vert X \Vert_{tr} \\
    \text { subject to } & A(X) = b\\ \nonumber
\end{array} 
\end{equation}

\subsection{Conic program}
\begin{defi}[Conic program]
  A conic program is an optimization problem of the form
  \[
  \begin{array}{ll}
      \min\limits_{x} & c^T x \\
      \text { subject to } & D(x) + d \in K \\ \nonumber
      & Ax = b
  \end{array} 
  \]
  where $c, x \in \mathbb{R}^n$, and $A\in \mathbb{R}^{m \times n}$, $b \in \mathbb{R}^m$. $D: \mathbb{R}^n \rightarrow Y$ is a linear map, $d \in Y$, for Euclidean space $Y$, $K \subseteq Y$ is a closed convex cone.
\end{defi}

\noindent Both LPs and SDPs are special cases of conic programming. For LPs, $K = \mathbb{R}^n_+$; for SDPs, $K = \mathbb{S}^n_+$.

\begin{defi}[Second-order cone program]
  A second-order cone program or SOCP is an optimization problem of the form
  \[
  \begin{array}{ll}
      \min\limits_{x} & c^T x \\
      \text { subject to } & \Vert D_i(x) + d_i \Vert_2 \leq e_i^T x + f_i, i=1, \cdots, p \\ \nonumber
      & Ax = b
  \end{array} 
  \]
\end{defi}

\noindent This is indeed a cone program. Recall the second-order cone $Q = \{(x,t): \Vert x \Vert_2 \leq t \}$, we have 
\begin{equation}
  \Vert D_i(x) + d_i \Vert_2 \leq e_i^T x + f_i \Longleftrightarrow (D_i(x) + d_i, e_i^T x + f_i) \in Q_i \nonumber
\end{equation}
for second-order cone $Q_i$ of appropriate dimensions. Now take $K = Q_1 \times \cdots \times Q_p$.

\noindent Observe that every LP is an SOCP and every SOCP is an SDP. Turns out that 
\begin{equation}
  \|x\|_{2} \leq t \Longleftrightarrow\left[\begin{array}{cc}t I & x \\x^{T} & t\end{array}\right] \succeq 0 \nonumber 
\end{equation}
hence any SOCP constraint can be written as an SDP constraint. The above is a special case of the Schur complement theorem: 
\begin{equation}
  \left[\begin{array}{cc}A & B \\B^{T} & C\end{array}\right] \succeq 0 \Longleftrightarrow A-B C^{-1} B^{T} \succeq 0 \nonumber
\end{equation}
for $A, C$ symmetric and $C \succ  0$.

\noindent In addition, QPs are SOCPs, which can be seen by rewriting a QP as
\begin{equation}
  \begin{array}{ll}
    \min\limits_{x, t} & c^T x + t \\
    \text { subject to } &  Dx \leq d, \frac{1}{2}x^TQx \leq t \\ \nonumber
    & Ax = b
\end{array} 
\end{equation}
now write $\frac{1}{2}x^TQx \leq t \Leftrightarrow  \left\|\left(\frac{1}{\sqrt{2}} Q^{1 / 2} x, \frac{1}{2}(1-t)\right)\right\|_{2} \leq \frac{1}{2}(1+t)$. Thus we have established the hierachy.
\begin{equation}
  \text{LPs} \subseteq \text{QPs} \subseteq \text{SOCPs} \subseteq \text{SDPs} \subseteq \text{Conic programs} \nonumber
\end{equation}

\section{Gradient Descent}
\subsection{Basic concept}
Consider the unconstrained, smooth convex optimization
\begin{equation}
  \min_x f(x) \nonumber
\end{equation}
where $f$ is convex and differentiable with $\text{dom}(f)=\mathbb{R}^n$. Denote optimal criterion value by $f^*= \min_x f(x)$, and solution by $x^*$.

\begin{defi}[Gradient descent] 
  Choose initial point $x^{(0)} \in \mathbb{R}^n$, repeat:
  \[
    x^{(k)} = x^{(k-1)} -t_k\cdot \nabla f(x^{(k-1)}), \ \ k=1,2,3,\ldots \nonumber
  \]
  Stop at some point.
\end{defi}

\noindent The second-order Taylor expansion of $f$ is
\begin{equation}
  f(y) \approx f(x) + \nabla f(x)^T(y-x) + \dfrac{1}{2}(y-x)\nabla^2 f(x)(y-x) \nonumber
\end{equation}

\noindent Consider the Quadratic approximation of $f$, replacing $\nabla^2 f(x)$ by $\tfrac{1}{t} I$ (replacing the curvature given by the Hessian with a much simpler notion of curvature - something spherical), we have 
\begin{equation}
  f(y) \approx f(x) + \nabla f(x)^T(y-x) + \dfrac{1}{2t}\Vert y-x \Vert^2 \nonumber
\end{equation}
where $f(x) + \nabla f(x)^T(y-x)$ is the linear approximation to $f$ and $\tfrac{1}{2t}\Vert y-x \Vert^2 \nonumber$ is the proximity term to $x$ with weight $\tfrac{1}{2t}$. This is a convex quadratic, we can minimize it by setting its gradient to zero, i.e.
\begin{equation}
  \dfrac{\partial f(y)}{ \partial f(x)} \approx \nabla f(x) + \dfrac{1}{t} (y-x) = 0 \\
  \Rightarrow y = x - t\nabla f(x)  \nonumber
\end{equation}
This gives us the gradient descent update rule. In other words, gradient descent actually chooses the next point to minimize this overall $y$. As shown in below figure, the blue point is moved to the point ont the curve directly below the red point.
\begin{figure}[htbp] 
  \centering 
  \includegraphics[width=0.4\textwidth]{img/gradient _descent _update.pdf} 
\end{figure}

\subsection{Step sizes}
\subsubsection{Fixed step size}
The simplest strategy is to take the step sizes $t_k$ to be fixed. However, if $t_k$ is too large, gradient descent can diverge, if $t_k$ is too small, gradient descent can be slow to converge.
\subsubsection{Backtracking line search}
One way to adaptively choose the step size is to use backtracking line search:
\begin{enumerate}
  \item Fix parameters $0 < \beta < 1$ and $0 < \alpha \leq \tfrac{1}{2}$
  \item At each iteration, start with $t = t_{init}$ (something relatively large), and while 
  \[
  f(x-t \nabla f(x)) > f(x) - \alpha t \Vert \nabla f(x) \Vert_2^2 \nonumber
  \]
  shirk $t = \beta t$. Else perform gradient descent update
  \[
  x^+ = x - t \nabla f(x)
  \] 
\end{enumerate}
The update criterion above denotes that if the progress we make by going from $x$ to $x - t\nabla f(x)$ is bigger than the progress we had $f(x) - \alpha t \Vert \nabla f(x)$, then we make $t$ smaller $\beta t$. This method is simple and tends to work well in practice (further simplification: just take $\alpha = \tfrac{1}{2}$).
\begin{figure}[htbp] 
  \centering 
  \includegraphics[width=0.6\textwidth]{img/backtracking_line_search.pdf} 
\end{figure}

\subsubsection{Exact line search}
We could also choose step to do the best we can along direction of negative gradient, called exact line search:
\begin{equation}
  t = \mathop{\arg\max}\limits_{s \geq 0} f(x - s\nabla f(x)) \nonumber
\end{equation}
Approximations to exact line search are typically not as efficient as backtracking, and it's typically not worth it.

\subsection{Convergence analysis}
\subsubsection{Gradient descent convergence}
Assume $f$ is convex and differentiable with $\text{dom}(f)=\mathbb{R}^n$, $\nabla f $ is Lipschitz continuous with constant $L \geq 0$, for any $x, y$:
\begin{equation}
  \Vert \nabla f(x) - \nabla f(y) \Vert_2 \leq L \Vert x-y \Vert_2 \nonumber
\end{equation}
If $f$ is twice differentiable:
\begin{equation}
   \nabla^2 f(x) \preceq L I \nonumber
\end{equation}
\begin{thm}
  Gradient descent with fixed step size $t \leq 1 / L$ satisfies
  \[
  f(x^{(k)}) - f^* \leq \dfrac{\Vert x^{(0)} - x^* \Vert_2^2}{2tk}
  \]
  and same result holds for backtracking, with $t$ replaced bt $\beta/ L$.
\end{thm}
We say gradient descent has converge rate $O(1/ k)$. That is, it finds $\epsilon$-suboptimal point in $O(1/ \epsilon)$ iterations.
\subsubsection{Analysis for strong convexity}
Assume Lipschitz gradient as before and $f$ has strong convexity:
\begin{thm}
  Gradient descent with fixed step size $t \leq 2 / (m+L)$ or with backtracking line search satisfies
  \[
  f(x^{(k)}) - f^* \leq \gamma^k \dfrac{L}{2}\Vert x^{(0)} - x^* \Vert_2^2 \nonumber
  \]
  where $0 < \gamma < 1$.
\end{thm}
\noindent Convergence rate under strong convexity is $O(\gamma^k)$, it finds $\epsilon$-suboptimal point in $O(log(1/ \epsilon))$ iterations. This is called linear convergence, because objective versus iteration curves loos linear on semi-log plot.

\noindent \textbf{Note}: denote $\gamma = O(1-m / L)$, thus convergence rate can be written as 
\begin{equation}
  O(\dfrac{L}{m} \log(1 / \epsilon)) \nonumber
\end{equation}
This means higher condition number $L / m$  will lead to slower rate. This is due to the Hessian being ellipsoidal and not spherical, so its optimization is slow. It is not only true of in theory, but also very apparent in practice too.

\noindent A look at the conditions for $f(\beta) = \dfrac{1}{2} \Vert y - X \beta \Vert_2^2$
\begin{enumerate}
  \item Lipschitz continuity of $\nabla f$:
  \begin{enumerate}
    \item $\nabla^2 f(x) \leq L I$
    \item As $\nabla^2 f(\beta) = X^T X$, we have $L = \lambda_{max}(X^T X)$
  \end{enumerate}
  \item Strong convexity of $f$:
  \begin{enumerate}
    \item $\nabla^2 f(x) \geq m I$
   \item As $\nabla^2 f(\beta) = X^T X$, we have $m = \lambda_{min}(X^T X)$
   \item If $X$ is wide ($X$ is $n \times p$ with $p > n$), then  $\lambda_{min} (X^T X) = 0$ and $f$ can not be strongly convex
   \item Even if $\delta_{min}(X) > 0$, we can have large condition number $\dfrac{L}{m} = \dfrac{\lambda_{max} (X^T X)}{\lambda_{min} (X^T X)}$
   \begin{enumerate}
    \item If there are correlated features, $L / m$ increases which leads to slow convergence
    \item If the features are orthogonal, $L / m = 1$ which leads to fast convergence 
   \end{enumerate} 
  \end{enumerate}
\end{enumerate}
\noindent \textbf{Note}: gradient descent always finds regularised solution to the under-parameterised problem.

\noindent Consider the least squares loss $f(\beta) = \dfrac{1}{2} \Vert y - X \beta \Vert_2^2$, the gradient descent update would be $\beta^{(k)} = \beta^{(k-1)} + t X^T (y - X \beta^{(k-1)})$. Suppose $p > n$, $X \beta = y$ has infinitely many solutions in $\overset{-}{\beta} + null(X)$. If we set $\beta^{(0)} = 0$, then the solution $\beta^{(k)}$ converges to argmin\{$\Vert \beta \Vert_2 : X\beta =y$\} as $k$ tends to $\infty$. The reason for this is that since we started in the row space of $X$, we will end in the row space of $X$.

\subsection{Practicalities}
Stopping rule: stop when $\Vert \nabla f(x) \Vert_2$ is small:
\begin{enumerate}
  \item $\nabla f(x^*) = 0$ at solution $x^*$
  \item If $f$ is strongly convex with $m$, $\Vert \nabla f(x) \Vert_2 \leq \sqrt[]{2m \epsilon} \Rightarrow f(x) - f^* \leq \epsilon$ 
\end{enumerate}
\noindent Pros and cons of gradient descent:
\begin{enumerate}
  \item Pro: simple idea, and each iteration is cheap (usually)
  \item Pro: fast for well-conditioned, strongly convex problems
  \item Con: can often be slow, because many interesting problems
  aren't strongly convex or well-conditioned
  \item Con: can't handle nondifferentiable functions
\end{enumerate}

\subsection{Nesterov acceleration}
Gradient descent has $O(1 / \epsilon)$ convergence rate over problem class of convex, differentiable functions with Lipschitz gradients.
First-order method: update $x^{(k)}$ iteratively
\begin{equation}
  x^{(0)} + \text{span} \{\nabla f(x^{(0)}), \nabla f(x^{(1)}), \cdots, \nabla f(x^{(k-1)}) \} \nonumber
\end{equation}
\begin{thm} \textbf{(Nesterov)} For any $k \leq \dfrac{n-1}{2}$ and any starting point $x^{(0)}$, there is a function $f$ in the
  problem class such that any first-order method satisfies:
  \[
    f(x^{(k)}) - f^* \geq \dfrac{3L \Vert x^{(0)} - x^* \Vert_2^2}{32(k+1)^2} \nonumber
  \]
\end{thm}
\noindent Gradient descent is a type of first-order method, which can be proved using induction. Since gradient descent converges at $O(1 / \epsilon)$, The above Theorem shows that there are more optimal methods than gradient descent, which converge at a rate of $O(1 / \sqrt[]{\epsilon} )$.

\subsection{Analysis for nonconvex case}
Assume $f$ is differentiable with Lipschitz gradient, now nonconvex. Instead of optimality, we settle for a $\epsilon$-substationary point solution, $\Vert \nabla f(x) \Vert_2 \leq \epsilon$
\begin{thm}
  Gradient descent with fixed step size $t \leq 1/ L$ satisfies:
  \[
    \mathop{\min}\limits_{i=0,\cdots,k} \Vert \nabla f(x^{(i)})\Vert_2 \leq \sqrt[]{\dfrac{2(f(x^{(0)}) - f^*)}{t(k+1)}} \nonumber
  \]
\end{thm}
Thus, the gradient descent has convergence rate $O(\dfrac{1}{\sqrt[]{k}})$ or $O(\dfrac{1}{\epsilon^2})$. This rate cannot be improved (over class of differentiable functions with Lipschitz gradients) by any deterministic algorithm.

\end{document}